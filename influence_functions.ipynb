{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIchud/FK2R7O5te8YzWrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nebulous05/influence-functions-pytorch/blob/main/influence_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### PROCEDURE ###\n",
        "\n",
        "# Step 1: Train a model on the 10-class MNIST dataset\n",
        "# Step 2: Arbitrarily select a wrongly-classified test point, z_test\n",
        "# Step 3: Compute the influence I_up,loss(z, z_test) for every training point z\n",
        "# Step 4: Select the 500 training points with the largest |I_up,loss(z, z_test)|\n",
        "# Step 5: Compute the actual change in test loss after removing the point and\n",
        "#         retraining for each of the 500 points\n",
        "# Step 6: Plot -1/n (I_up,loss(z, z_test)) vs. actual change in test loss for\n",
        "#         each of the 500 points"
      ],
      "metadata": {
        "id": "J2QDScO9mR5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Train a model on the 10-class MNIST dataset"
      ],
      "metadata": {
        "id": "mVuujdT-oNOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cwQH8J4ipnyh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data is torch.FloatTensor; shape = (1, 28, 28)\n",
        "# load train data: 60,000 samples\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "#loading test data: 10,000 samples\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "0VzLbKHKpszA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116e766e-f8aa-4c06-8dd5-2d6b24f1dbd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 18450647.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 604285.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1528346.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3425737.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_item(index, dataset='train'):\n",
        "    ''' returns a pair (image_tensor, label_int) '''\n",
        "    if dataset == 'train':\n",
        "        return train_dataset[index]\n",
        "    elif dataset == 'test':\n",
        "        return test_dataset[index]"
      ],
      "metadata": {
        "id": "306JWBg3jXg9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(index, dataset='train'):\n",
        "    ''' show an image with matplotlib '''\n",
        "    img = train_dataset[index][0].numpy().reshape(28, 28)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VUtugS-Tkmna"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load train and test data samples into dataloader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "3-6Gz7oslNka"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom module for logistic regression\n",
        "class LRModel(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super(LRModel, self).__init__()\n",
        "        self.linear = nn.Linear(n_inputs, n_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "_s9RXb1Elzs2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model\n",
        "n_inputs = 28*28\n",
        "n_outputs = 10\n",
        "model = LRModel(n_inputs, n_outputs)\n",
        "\n",
        "# defining the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# defining Cross-Entropy loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "qfbNFl2ImU5F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "def train(model=model, epochs=50, training_loader=train_loader, testing_loader=test_loader):\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      for i, (images, labels) in enumerate(training_loader):\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images.view(-1, 28*28))\n",
        "          loss = loss_fn(outputs, labels)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      correct = 0\n",
        "      for images, labels in testing_loader:\n",
        "          outputs = model(images.view(-1, 28*28))\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct += (predicted == labels).sum()\n",
        "      accuracy = 100 * (correct.item()) / len(test_dataset)\n",
        "      accuracies.append(accuracy)\n",
        "      print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch, loss.item(), accuracy))"
      ],
      "metadata": {
        "id": "PpoXOYS9m5_Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_eFX0_7qmFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
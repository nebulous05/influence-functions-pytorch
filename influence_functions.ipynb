{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuVZ3mZ1cryBFEkEbJNW9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nebulous05/influence-functions-pytorch/blob/main/influence_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKBl5oYXwcki"
      },
      "outputs": [],
      "source": [
        "### PROCEDURE ###\n",
        "\n",
        "# Step 1: Train a model on the 10-class MNIST dataset\n",
        "# Step 2: Arbitrarily select a wrongly-classified test point, z_test\n",
        "# Step 3: Compute the influence I_up,loss(z, z_test) for every training point z\n",
        "# Step 4: Select the 500 training points with the largest |I_up,loss(z, z_test)|\n",
        "# Step 5: Compute the actual change in test loss after removing the point and\n",
        "#         retraining for each of the 500 points\n",
        "# Step 6: Plot -1/n (I_up,loss(z, z_test)) vs. actual change in test loss for\n",
        "#         each of the 500 points"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Arbitrarily select a wrongly-classified test point, z_test"
      ],
      "metadata": {
        "id": "RaCfGKqRwjI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.autograd import grad"
      ],
      "metadata": {
        "id": "7F1A0iOpwm8o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data is torch.FloatTensor; shape = (1, 28, 28)\n",
        "# load train data: 60,000 samples\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "#loading test data: 10,000 samples\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmn3n0bpwpwI",
        "outputId": "22f103dd-0d0e-4d00-b605-1951197f504f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5455731.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 157984.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1296625.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2704888.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_item(index, dataset='train'):\n",
        "    ''' returns a pair (image_tensor, label_int) '''\n",
        "    if dataset == 'train':\n",
        "        return train_dataset[index]\n",
        "    elif dataset == 'test':\n",
        "        return test_dataset[index]\n",
        "\n",
        "def show_image(index, dataset='train'):\n",
        "    ''' show an image with matplotlib '''\n",
        "    if dataset == 'train':\n",
        "        img = train_dataset[index][0].numpy().reshape(28, 28)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.show()\n",
        "    elif dataset == 'test':\n",
        "        img = test_dataset[index][0].numpy().reshape(28, 28)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "sISqw3Bjwve_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load train and test data samples into dataloader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Pp6y3-vJw2av"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom module for logistic regression\n",
        "class LRModel(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        super(LRModel, self).__init__()\n",
        "        self.linear = nn.Linear(n_inputs, n_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "14I5FLzyw5nX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model\n",
        "n_inputs = 28*28\n",
        "n_outputs = 10\n",
        "model = LRModel(n_inputs, n_outputs)\n",
        "model.load_state_dict(torch.load('model_weights.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH-9cY1Fw706",
        "outputId": "dcb3e1d4-9525-4f28-8fec-60ed37c39b96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ac56d928a54d>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model_weights.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def find_misclassified_indices(model=model, test_dataset=test_dataset):\n",
        "  ''' returns a list of indices of all incorrectly labeled test points '''\n",
        "  model.eval()\n",
        "  indices = []\n",
        "  for i in range(len(test_dataset)):\n",
        "    image, label = get_item(i, dataset='test')\n",
        "    image = image.view(-1, 28*28)\n",
        "    with torch.no_grad():\n",
        "      y_pred = model(image)\n",
        "      _, predicted = torch.max(y_pred.data, 1)\n",
        "      if predicted != label:\n",
        "        indices.append(i)\n",
        "  return indices\n",
        "\n",
        "mislabeled_indices = find_misclassified_indices()"
      ],
      "metadata": {
        "id": "x7gVRrERxJqu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(random.choice(mislabeled_indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSFnT8fJQBkG",
        "outputId": "f3589751-25d0-4cf6-8c02-754165849500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_label = get_item(7620, dataset='test')\n",
        "z_test = image_label[0].view(-1, 28*28)\n",
        "z_test_label = torch.tensor([train_dataset[7620][1]])\n",
        "\n",
        "print(\"label: \", image_label[1])\n",
        "show_image(7620)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "_Z6mEJy2QvBt",
        "outputId": "7b708227-29e9-47cb-e958-821e1e839e58"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label:  3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Compute the influence I_up,loss(z, z_test) for every training point z"
      ],
      "metadata": {
        "id": "Yy1HW-rgQxa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_indices = [1,2,3]\n",
        "train_batch = [train_dataset[i] for i in batch_indices]\n",
        "train_batch_inputs = torch.stack([x[0].view(-1, 28*28).squeeze() for x in train_batch])\n",
        "train_batch_labels = torch.tensor([x[1] for x in train_batch])\n",
        "\n",
        "# model.zero_grad()\n",
        "# output = model(train_batch_inputs)\n",
        "# loss = loss_fn(output, train_batch_labels)\n",
        "\n",
        "# print(output.shape)\n",
        "# print(train_batch_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9riTP564sl0w",
        "outputId": "e1689a1a-3731-4290-c2e6-cd643cea7de4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10])\n",
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get gradient of loss wrt params to kick off LiSSA\n",
        "def compute_v(model, z_test, z_test_label, loss_fn):\n",
        "    model.zero_grad()\n",
        "    output = model(z_test)\n",
        "    loss = loss_fn(output, z_test_label)\n",
        "\n",
        "    v = grad(loss, model.parameters())\n",
        "    return v\n",
        "\n",
        "# returns list of torch tensors containing product of Hessian and v.\n",
        "def hvp(y, w, v):\n",
        "    # First backprop: Compute the gradient of y with respect to w\n",
        "    first_grads = grad(y, w, retain_graph=True, create_graph=True)\n",
        "\n",
        "    # Elementwise products\n",
        "    elemwise_products = 0\n",
        "    for grad_elem, v_elem in zip(first_grads, v):\n",
        "        elemwise_products += torch.sum(grad_elem * v_elem)\n",
        "\n",
        "    # Second backprop: Compute gradient of elementwise product with respect to w\n",
        "    return_grads = grad(elemwise_products, w, create_graph=True)\n",
        "\n",
        "    return return_grads\n",
        "\n",
        "# Precompute s_test using stochastic estimation, now using a mini-batch from the training data\n",
        "def compute_s_test(model, z_test, v, loss_fn, scale=25, recursion_depth=5000, damping=0, batch_size=1, num_samples=10):\n",
        "    model.eval()\n",
        "    s_test = None\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        h_estimate = v  # Start with the gradient of z_test\n",
        "\n",
        "        for i in range(recursion_depth):\n",
        "            # Sample a random mini-batch from the training data\n",
        "            batch_indices = random.sample(range(len(train_dataset)), batch_size)\n",
        "            train_batch = [train_dataset[i] for i in batch_indices]\n",
        "            train_batch_inputs = torch.stack([x[0].view(-1, 28*28).squeeze() for x in train_batch])\n",
        "            train_batch_labels = torch.tensor([x[1] for x in train_batch])\n",
        "\n",
        "            model.zero_grad()\n",
        "            output = model(train_batch_inputs)\n",
        "            loss = loss_fn(output, train_batch_labels)\n",
        "\n",
        "            # Compute the Hessian-vector product\n",
        "            hv = hvp(loss, list(model.parameters()), h_estimate)\n",
        "\n",
        "            # Recursively update h_estimate\n",
        "            h_estimate = [\n",
        "                _v + (1 - damping) * _h_e - _hv / scale\n",
        "                for _v, _h_e, _hv in zip(v, h_estimate, hv)\n",
        "            ]\n",
        "\n",
        "        if s_test is None:\n",
        "            s_test = h_estimate\n",
        "        else:\n",
        "            s_test = [s + h for s, h in zip(s_test, h_estimate)]\n",
        "\n",
        "    # Average over the number of samples\n",
        "    s_test = [s / num_samples for s in s_test]\n",
        "\n",
        "    return s_test"
      ],
      "metadata": {
        "id": "e3zKOynsS9VI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = compute_v(model, z_test, z_test_label, loss_fn)\n",
        "s_test = compute_s_test(model, z_test, v, loss_fn, recursion_depth=10, num_samples=1)\n",
        "\n",
        "print(s_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OdpQczx6P1h",
        "outputId": "86a07e6a-41c5-42d9-eb80-8732f7627685"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<DivBackward0>), tensor([ 0.0167,  0.0057,  0.0045,  0.3037,  0.0918,  0.2346,  0.0062,  0.0256,\n",
            "         0.1686, -2.1770], grad_fn=<DivBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = compute_v(model, z_test, z_test_label, loss_fn)\n",
        "\n",
        "h_estimate = v\n",
        "damping = 0\n",
        "scale = 25\n",
        "\n",
        "print(h_estimate)\n",
        "\n",
        "\n",
        "batch_indices = [1,2,3]\n",
        "train_batch = [train_dataset[i] for i in batch_indices]\n",
        "train_batch_inputs = torch.stack([x[0].view(-1, 28*28).squeeze() for x in train_batch])\n",
        "train_batch_labels = torch.tensor([x[1] for x in train_batch])\n",
        "\n",
        "model.zero_grad()\n",
        "output = model(train_batch_inputs)\n",
        "loss = loss_fn(output, train_batch_labels)\n",
        "\n",
        "# Compute the Hessian-vector product\n",
        "hv = hvp(loss, list(model.parameters()), h_estimate)\n",
        "\n",
        "\n",
        "# # Recursively update h_estimate\n",
        "h_estimate = [\n",
        "    _v + (1 - damping) * _h_e - _hv / scale\n",
        "    for _v, _h_e, _hv in zip(v, h_estimate, hv)\n",
        "]\n",
        "\n",
        "print(h_estimate)\n",
        "\n",
        "# print(list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VebdWpuJy1Ju",
        "outputId": "2fa0c907-f91e-459a-c82d-0334e77992ba"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [-0., -0., -0.,  ..., -0., -0., -0.]]), tensor([ 0.0016,  0.0007,  0.0005,  0.0287,  0.0084,  0.0224,  0.0006,  0.0027,\n",
            "         0.0164, -0.2074]))\n",
            "[tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [-0., -0., -0.,  ..., -0., -0., -0.]], grad_fn=<SubBackward0>), tensor([ 0.0031,  0.0014,  0.0010,  0.0572,  0.0166,  0.0447,  0.0013,  0.0054,\n",
            "         0.0328, -0.4143], grad_fn=<SubBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rlF-xR-334gI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}